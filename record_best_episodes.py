#!/usr/bin/env python3
"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –∑–∞–ø–∏—Å–∏ –ª—É—á—à–∏—Ö —ç–ø–∏–∑–æ–¥–æ–≤ AMAL –≤ replay format
"""

import os
import sys
from pathlib import Path
import yaml
import torch
import numpy as np
from datetime import datetime
import shutil

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ PyMARL –≤ sys.path
PYMARL_PATH = str(Path(__file__).resolve().parent / "pymarl" / "src")
if PYMARL_PATH not in sys.path:
    sys.path.append(PYMARL_PATH)

from smac.env import StarCraft2Env
from core.amal_agent import AMALAgent

class ReplayRecorder:
    """–ö–ª–∞—Å—Å –¥–ª—è –∑–∞–ø–∏—Å–∏ –∏ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏ replay —Ñ–∞–π–ª–æ–≤"""
    
    def __init__(self, scenario_name: str, model_path: str = None):
        self.scenario_name = scenario_name
        self.model_path = model_path
        self.replay_dir = "collected_replays"
        os.makedirs(self.replay_dir, exist_ok=True)
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
        with open('configs/amal_config.yaml', 'r') as f:
            self.config = yaml.safe_load(f)
    
    def record_episode(self, episode_type: str = "demo", max_steps: int = None):
        """–ó–∞–ø–∏—Å—ã–≤–∞–µ—Ç –æ–¥–∏–Ω —ç–ø–∏–∑–æ–¥ —Å replay"""
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        replay_prefix = f"amal_{self.scenario_name}_{episode_type}_{timestamp}"
        
        # –°–æ–∑–¥–∞–µ–º SMAC environment —Å replay
        env = StarCraft2Env(
            map_name=self.scenario_name,
            seed=42,
            replay_dir="/tmp",  # –í—Ä–µ–º–µ–Ω–Ω–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è
            replay_prefix=replay_prefix
        )
        
        env_info = env.get_env_info()
        n_agents = env_info["n_agents"]
        obs_dim = env_info["obs_shape"]
        state_dim = env_info["state_shape"]
        action_dim = env_info["n_actions"]
        episode_limit = env_info["episode_limit"]
        
        if max_steps is None:
            max_steps = episode_limit
        
        # –°–æ–∑–¥–∞–µ–º –∞–≥–µ–Ω—Ç–∞
        agent = AMALAgent(
            n_agents=n_agents,
            obs_dim=obs_dim,
            state_dim=state_dim,
            action_dim=action_dim,
            config=self.config['algorithm'],
            device='cuda' if torch.cuda.is_available() else 'cpu'
        )
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω–∞
        use_trained = False
        if self.model_path and os.path.exists(self.model_path):
            try:
                agent.load(self.model_path)
                use_trained = True
                print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–∞ –º–æ–¥–µ–ª—å: {self.model_path}")
            except Exception as e:
                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}")
        
        # –ó–∞–ø—É—Å–∫–∞–µ–º —ç–ø–∏–∑–æ–¥
        env.reset()
        obs_list = env.get_obs()
        state = env.get_state()
        obs = {i: np.array(obs_list[i]) for i in range(n_agents)}
        
        done = False
        episode_reward = 0
        episode_steps = 0
        
        print(f"üéÆ –ó–∞–ø–∏—Å—å —ç–ø–∏–∑–æ–¥–∞ '{episode_type}' –¥–ª—è {self.scenario_name}...")
        
        action_history = []
        reward_history = []
        
        while not done and episode_steps < max_steps:
            # –ü–æ–ª—É—á–∞–µ–º –¥–æ—Å—Ç—É–ø–Ω—ã–µ –¥–µ–π—Å—Ç–≤–∏—è
            avail_actions_list = env.get_avail_actions()
            avail_actions = {i: np.array(avail_actions_list[i]) for i in range(n_agents)}
            
            if use_trained:
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å
                actions, info = agent.select_actions(obs, state, avail_actions, explore=False)
            else:
                # –°–ª—É—á–∞–π–Ω—ã–µ –¥–µ–π—Å—Ç–≤–∏—è
                actions = {}
                for i in range(n_agents):
                    avail_indices = np.where(avail_actions[i] == 1)[0]
                    if len(avail_indices) > 0:
                        actions[i] = np.random.choice(avail_indices)
                    else:
                        actions[i] = 0
            
            # –í—ã–ø–æ–ª–Ω—è–µ–º —à–∞–≥
            action_list = [actions.get(i, 0) for i in range(n_agents)]
            reward, done, env_info = env.step(action_list)
            
            # –û–±–Ω–æ–≤–ª—è–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ
            next_obs_list = env.get_obs()
            next_state = env.get_state()
            obs = {i: np.array(next_obs_list[i]) for i in range(n_agents)}
            state = np.array(next_state)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏—Å—Ç–æ—Ä–∏—é
            action_history.append(actions.copy())
            reward_history.append(reward)
            
            episode_reward += reward
            episode_steps += 1
            
            if episode_steps % 20 == 0:
                print(f"  –®–∞–≥ {episode_steps}: Actions={actions}, Reward={reward:.2f}")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º replay
        try:
            env.save_replay()
            replay_saved = True
            print("‚úÖ Replay —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ SMAC")
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è replay: {e}")
            replay_saved = False
        
        env.close()
        
        # –ò—â–µ–º —Å–æ–∑–¥–∞–Ω–Ω—ã–π —Ñ–∞–π–ª replay
        replay_file = None
        if replay_saved:
            # –ò—â–µ–º —Ñ–∞–π–ª –≤ –≤–æ–∑–º–æ–∂–Ω—ã—Ö –º–µ—Å—Ç–∞—Ö
            search_paths = [
                f"/home/user/StarCraftII/Replays/{replay_prefix}*.SC2Replay",
                f"/tmp/{replay_prefix}*.SC2Replay",
                f"/home/user/StarCraftII/Replays/*/{replay_prefix}*.SC2Replay"
            ]
            
            import glob
            for pattern in search_paths:
                files = glob.glob(pattern)
                if files:
                    replay_file = files[0]
                    break
        
        # –ö–æ–ø–∏—Ä—É–µ–º replay –≤ –Ω–∞—à—É –ø–∞–ø–∫—É
        final_replay_path = None
        if replay_file and os.path.exists(replay_file):
            final_name = f"{replay_prefix}_r{episode_reward:.2f}_s{episode_steps}.SC2Replay"
            final_replay_path = os.path.join(self.replay_dir, final_name)
            shutil.copy2(replay_file, final_replay_path)
            print(f"üìÅ Replay —Å–∫–æ–ø–∏—Ä–æ–≤–∞–Ω –≤: {final_replay_path}")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        metadata = {
            'episode_type': episode_type,
            'scenario': self.scenario_name,
            'model_path': self.model_path,
            'use_trained': use_trained,
            'episode_steps': episode_steps,
            'episode_reward': episode_reward,
            'battle_won': env_info.get('battle_won', False),
            'timestamp': timestamp,
            'replay_file': final_replay_path,
            'action_history': action_history[:10],  # –ü–µ—Ä–≤—ã–µ 10 –¥–µ–π—Å—Ç–≤–∏–π –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            'reward_history': reward_history[:10]
        }
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º JSON —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
        import json
        metadata_file = os.path.join(self.replay_dir, f"{replay_prefix}_metadata.json")
        with open(metadata_file, 'w') as f:
            json.dump(metadata, f, indent=2, default=str)
        
        return metadata

def record_multiple_episodes():
    """–ó–∞–ø–∏—Å—ã–≤–∞–µ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ —ç–ø–∏–∑–æ–¥–æ–≤ —Ä–∞–∑–Ω—ã—Ö —Ç–∏–ø–æ–≤"""
    
    print("üé¨ === –ú–ê–°–°–û–í–ê–Ø –ó–ê–ü–ò–°–¨ REPLAY –≠–ü–ò–ó–û–î–û–í ===")
    
    scenarios = ["2s_vs_1sc"]  # –ù–∞—á–Ω–µ–º —Å –ø—Ä–æ—Å—Ç–æ–≥–æ
    model_paths = [
        "models/amal_2s_vs_1sc_e100.pt",
        None  # –°–ª—É—á–∞–π–Ω–∞—è –ø–æ–ª–∏—Ç–∏–∫–∞ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
    ]
    
    recorder_results = []
    
    for scenario in scenarios:
        for i, model_path in enumerate(model_paths):
            model_type = "trained" if model_path else "random"
            
            print(f"\nüìπ –ó–∞–ø–∏—Å—ã–≤–∞–µ–º {scenario} —Å {model_type} –º–æ–¥–µ–ª—å—é...")
            
            recorder = ReplayRecorder(scenario, model_path)
            
            # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ —ç–ø–∏–∑–æ–¥–æ–≤
            for episode_num in range(3):  # 3 —ç–ø–∏–∑–æ–¥–∞ –∫–∞–∂–¥–æ–≥–æ —Ç–∏–ø–∞
                try:
                    metadata = recorder.record_episode(
                        episode_type=f"{model_type}_{episode_num}",
                        max_steps=200  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª—è –±—ã—Å—Ç—Ä–æ—Ç—ã
                    )
                    recorder_results.append(metadata)
                    
                    print(f"  ‚úÖ –≠–ø–∏–∑–æ–¥ {episode_num}: R={metadata['episode_reward']:.2f}, Steps={metadata['episode_steps']}")
                    
                except Exception as e:
                    print(f"  ‚ùå –û—à–∏–±–∫–∞ –≤ —ç–ø–∏–∑–æ–¥–µ {episode_num}: {e}")
    
    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç—á–µ—Ç
    print(f"\nüìä === –û–¢–ß–ï–¢ –û –ó–ê–ü–ò–°–ê–ù–ù–´–• REPLAY ===")
    print(f"–í—Å–µ–≥–æ –∑–∞–ø–∏—Å–∞–Ω–æ —ç–ø–∏–∑–æ–¥–æ–≤: {len(recorder_results)}")
    
    for result in recorder_results:
        print(f"  {result['episode_type']}: R={result['episode_reward']:.2f}, "
              f"Steps={result['episode_steps']}, Won={result['battle_won']}")
    
    print(f"\nüìÅ –í—Å–µ replay —Ñ–∞–π–ª—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: collected_replays/")
    print(f"\nüéÆ –ö–∞–∫ –ø—Ä–æ—Å–º–æ—Ç—Ä–µ—Ç—å:")
    print(f"  1. –°–∫–æ–ø–∏—Ä—É–π—Ç–µ .SC2Replay —Ñ–∞–π–ª—ã –≤ StarCraft II/Replays/")
    print(f"  2. –û—Ç–∫—Ä–æ–π—Ç–µ StarCraft II ‚Üí Replays")
    print(f"  3. –ê–Ω–∞–ª–∏–∑–∏—Ä—É–π—Ç–µ –ø–æ–≤–µ–¥–µ–Ω–∏–µ –∞–≥–µ–Ω—Ç–æ–≤!")
    
    return recorder_results

if __name__ == "__main__":
    try:
        results = record_multiple_episodes()
        print(f"\nüéâ –ó–∞–ø–∏—Å—å –∑–∞–≤–µ—Ä—à–µ–Ω–∞! –ó–∞–ø–∏—Å–∞–Ω–æ {len(results)} —ç–ø–∏–∑–æ–¥–æ–≤.")
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
        import traceback
        traceback.print_exc()
